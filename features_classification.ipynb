{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification based on leaves features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing our own functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import src.Data as Data\n",
    "importlib.reload(Data)\n",
    "Data = Data.Data\n",
    "\n",
    "import src.Metrics as Metrics\n",
    "importlib.reload(Metrics)\n",
    "Metrics = Metrics.Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "`numpy` and `pandas` are used to manipulate the data\n",
    "\n",
    "`scikit-learn` is used to train the classification models and compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve, cross_validate, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(), \n",
    "    RandomForestClassifier(n_jobs=-1), \n",
    "    #BaggingClassifier(n_jobs=-1), \n",
    "    LogisticRegression(n_jobs=-1),\n",
    "    #SVC(),\n",
    "    #GaussianNB(),\n",
    "    #SGDClassifier(n_jobs=-1),\n",
    "    #KNeighborsClassifier(n_jobs=-1),\n",
    "    #GradientBoostingClassifier(),\n",
    "    #MLPClassifier(),\n",
    "    #AdaBoostClassifier()\n",
    "    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The data is loaded from the `data` folder.\n",
    "\n",
    "Samples are split into a training set and a test set with a custom ratio. Stratified sampling is used to ensure that the proportion of samples in each class is the same in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least populated class count: 8\n"
     ]
    }
   ],
   "source": [
    "data: Data = Data(test_size=0.2, include_images=False)\n",
    "\n",
    "least_populated_class_count = np.unique(data.y_train, return_counts=True)[1].min()\n",
    "print(\"Least populated class count:\", least_populated_class_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models\n",
    "\n",
    "#### _Without cross-validation_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We first train the models on the training set without cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit the model without cross-validation or hyperparameter tuning. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fit the model without cross-validation or hyperparameter tuning. \\n')\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(data.x_train, data.y_train)\n",
    "    predictions = classifier.predict(data.x_test)\n",
    "\n",
    "    classifier_metrics = Metrics(classifier_name=classifier.__class__.__name__)\n",
    "    # Computing metrics\n",
    "    classifier_metrics.add_metric('accuracy_score', accuracy_score(data.y_test, predictions))\n",
    "    classifier_metrics.add_metric('f1_score', f1_score(data.y_test, predictions, average='macro'))\n",
    "    classifier_metrics.add_metric('precision_score', precision_score(data.y_test, predictions, average='macro'))\n",
    "    classifier_metrics.add_metric('recall_score', recall_score(data.y_test, predictions, average='macro'))\n",
    "\n",
    "    metrics_list.append(classifier_metrics)\n",
    "\n",
    "    Metrics.show_metrics_list(metrics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _With cross-validation_\n",
    "We then train the models on the training set with cross-validation.\n",
    "\n",
    "The number of folds is chosen to be the number of samples in the least represented class to ensure that each fold contains at least one sample of each class. (This is automatically done by `sklearn.model_selection.StratifiedKFold` used by the function `cross_validate`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fit the model with cross-validation.\\n')\n",
    "\n",
    "metrics_list_cv = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    scores = cross_validate(classifier, data.x_tab, data.y_tab, cv=least_populated_class_count, n_jobs=-1, return_train_score=True, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "\n",
    "    classifier_metrics = Metrics(classifier_name=classifier.__class__.__name__)\n",
    "    # Storing metrics\n",
    "    classifier_metrics.add_metric('accuracy_score', scores['test_accuracy'].mean())\n",
    "    classifier_metrics.add_metric('f1_score', scores['test_f1_macro'].mean())\n",
    "    classifier_metrics.add_metric('precision_score', scores['test_precision_macro'].mean())\n",
    "    classifier_metrics.add_metric('recall_score', scores['test_recall_macro'].mean())\n",
    "\n",
    "    metrics_list_cv.append(classifier_metrics)\n",
    "\n",
    "Metrics.show_metrics_list(metrics_list_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _With cross-validation and hyperparameter tuning_\n",
    "We then train the models on the training set with cross-validation and using the best hyperparameters found using the dedicated notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = []\n",
    "\n",
    "selected_classifiers = [classifier.__class__.__name__ for classifier in classifiers]\n",
    "\n",
    "for i, clf_name in enumerate(selected_classifiers):\n",
    "    if clf_name == 'DecisionTreeClassifier':\n",
    "        classifiers[i].set_params(**{\n",
    "            'criterion': 'gini',\n",
    "            'max_depth': None,\n",
    "            'max_features': 'sqrt',\n",
    "            'min_samples_leaf': 1,\n",
    "            'min_samples_split': 5,\n",
    "            'splitter': 'best',\n",
    "            })\n",
    "        \n",
    "    if clf_name == 'RandomForestClassifier':\n",
    "        classifiers[i].set_params(**{\n",
    "            'criterion': 'gini',\n",
    "            'max_features': 'log2',\n",
    "            'min_samples_split': 2,\n",
    "            'n_estimators': 500,\n",
    "            })\n",
    "\n",
    "    if clf_name == 'BaggingClassifier':\n",
    "        classifiers[i].set_params(**{\n",
    "            'bootstrap': False,\n",
    "            'bootstrap_features': True,\n",
    "            'max_features': 0.1,\n",
    "            'max_samples': 0.5,\n",
    "            'n_estimators': 100,\n",
    "            })\n",
    "\n",
    "    if clf_name == 'LogisticRegression':\n",
    "        classifiers[i].set_params(**{\n",
    "            'C': 1000,\n",
    "            'max_iter': 100,\n",
    "            'penalty': 'l2',\n",
    "            'solver': 'liblinear',\n",
    "            })\n",
    "\n",
    "    if clf_name == 'SVC':\n",
    "        classifiers[i].set_params(**{\n",
    "            'C': 50,\n",
    "            'gamma': 'scale',\n",
    "            'kernel': 'linear',\n",
    "            })\n",
    "\n",
    "    if clf_name == 'GaussianNB':\n",
    "        classifiers[i].set_params(**{\n",
    "            'var_smoothing': 0.005,\n",
    "            })\n",
    "\n",
    "    if clf_name == 'SGDClassifier':\n",
    "        classifiers[i].set_params(**{\n",
    "            'alpha': 0.0001,\n",
    "            'loss': 'modified_huber',\n",
    "            'max_iter': 2000,\n",
    "            'penalty': 'l1',\n",
    "            })\n",
    "\n",
    "    if clf_name == 'KNeighborsClassifier':\n",
    "        classifiers[i].set_params(**{\n",
    "            'algorithm': 'auto',\n",
    "            'leaf_size': 1,\n",
    "            'n_neighbors': 1,\n",
    "            'p': 1,\n",
    "            'weights': 'distance',\n",
    "            })\n",
    "\n",
    "    if clf_name == 'GradientBoostingClassifier':\n",
    "        classifiers[i].set_params(**{\n",
    "            'criterion': 'friedman_mse',\n",
    "            'learning_rate': 0.025,\n",
    "            'max_depth': 2,\n",
    "            'max_features': 'log2',\n",
    "            'min_samples_split': 10,\n",
    "            'n_estimators': 500,\n",
    "            })\n",
    "\n",
    "    if clf_name == 'MLPClassifier':\n",
    "        classifiers[i].set_params(**{\n",
    "            'activation': 'logistic',\n",
    "            'alpha': 0.01,\n",
    "            'hidden_layer_sizes': (100,),\n",
    "            'learning_rate': 'adaptive',\n",
    "            'max_iter': 500,\n",
    "            'solver': 'lbfgs',\n",
    "            })\n",
    "\n",
    "    if clf_name == 'AdaBoostClassifier':\n",
    "        classifiers[i].set_params(**{\n",
    "            'algorithm': 'SAMME.R',\n",
    "            'learning_rate': 0.01,\n",
    "            'n_estimators': 500,\n",
    "            })\n",
    "\n",
    "print('Fit the model with cross-validation and the best hyperparameters.\\n')\n",
    "\n",
    "metrics_list_cv = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    scores = cross_validate(classifier, data.x_tab, data.y_tab, cv=least_populated_class_count, n_jobs=-1, return_train_score=True, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "\n",
    "    classifier_metrics = Metrics(classifier_name=classifier.__class__.__name__)\n",
    "    # Storing metrics\n",
    "    classifier_metrics.add_metric('accuracy_score', scores['test_accuracy'].mean())\n",
    "    classifier_metrics.add_metric('f1_score', scores['test_f1_macro'].mean())\n",
    "    classifier_metrics.add_metric('precision_score', scores['test_precision_macro'].mean())\n",
    "    classifier_metrics.add_metric('recall_score', scores['test_recall_macro'].mean())\n",
    "\n",
    "    metrics_list_cv.append(classifier_metrics)\n",
    "\n",
    "Metrics.show_metrics_list(metrics_list_cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
